import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                           f1_score, precision_recall_fscore_support, roc_curve, auc)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy import stats
from statsmodels.stats.contingency_tables import mcnemar
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import torch.nn.functional as F

warnings.filterwarnings('ignore')

# ================================
# MOD√àLE UNI-MODAL 
# ================================

class UniModalModel(nn.Module):
    def __init__(self, input_dim, hidden_dims=[256, 128], dropout_rate=0.3, use_attention=False):
        super(UniModalModel, self).__init__()

        self.use_attention = use_attention

        if input_dim > 1000:
            hidden_dims = [512, 256, 128]
        elif input_dim > 500:
            hidden_dims = [256, 128, 64]
        elif input_dim > 200:
            hidden_dims = [128, 64]
        else:
            hidden_dims = [64, 32]

        layers = []
        prev_dim = input_dim

        for i, hidden_dim in enumerate(hidden_dims):
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim

        self.feature_extractor = nn.Sequential(*layers)

        if self.use_attention:
            self.attention = nn.Sequential(
                nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),
                nn.ReLU(),
                nn.Linear(hidden_dims[-1] // 2, 1),
                nn.Sigmoid()
            )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(hidden_dims[-1] // 2, 2)
        )

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.BatchNorm1d):
            nn.init.constant_(module.weight, 1)
            nn.init.constant_(module.bias, 0)

    def forward(self, x):
        features = self.feature_extractor(x)

        if self.use_attention:
            attention_weights = self.attention(features)
            features = features * attention_weights

        output = self.classifier(features)

        return output


# ================================
# BLOCS POUR TEACHER 
# ================================

class ResidualBlock(nn.Module):
    """Bloc r√©siduel avec skip connection"""
    def __init__(self, in_dim, out_dim, dropout_rate=0.3):
        super(ResidualBlock, self).__init__()

        self.linear = nn.Linear(in_dim, out_dim)
        self.bn = nn.BatchNorm1d(out_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate)

        self.skip = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()

    def forward(self, x):
        identity = self.skip(x)

        out = self.linear(x)
        out = self.bn(out)
        out = self.relu(out)
        out = self.dropout(out)

        out = out + identity
        return out


class FeedForwardNetwork(nn.Module):
    """FFN comme dans Transformer"""
    def __init__(self, dim, expansion_factor=2, dropout_rate=0.3):
        super(FeedForwardNetwork, self).__init__()

        hidden_dim = dim * expansion_factor

        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout_rate)
        )

    def forward(self, x):
        return self.net(x)


# ================================
# MOD√àLE √Ä DOUBLE BRANCHE (FLEXIBLE)
# ================================

class DualBranchModel(nn.Module):
    def __init__(self, code_dim, commit_dim, metrics_dim, syntax_dim,
                 hidden_dims=[256, 128], dropout_rate=0.3,
                 main_modalities=['code', 'metrics', 'syntax'],
                 teacher_modalities=['code', 'commit', 'metrics', 'syntax']):
        super(DualBranchModel, self).__init__()

        self.code_dim = code_dim
        self.commit_dim = commit_dim
        self.metrics_dim = metrics_dim
        self.syntax_dim = syntax_dim
        self.main_modalities = main_modalities
        self.teacher_modalities = teacher_modalities

        # Calcul dynamique des dimensions selon les modalit√©s
        main_branch_dim = sum([
            code_dim if 'code' in main_modalities else 0,
            commit_dim if 'commit' in main_modalities else 0,
            metrics_dim if 'metrics' in main_modalities else 0,
            syntax_dim if 'syntax' in main_modalities else 0
        ])

        teacher_branch_dim = sum([
            code_dim if 'code' in teacher_modalities else 0,
            commit_dim if 'commit' in teacher_modalities else 0,
            metrics_dim if 'metrics' in teacher_modalities else 0,
            syntax_dim if 'syntax' in teacher_modalities else 0
        ])

        if main_branch_dim > 1000:
            hidden_dims = [512, 256, 128]
        elif main_branch_dim > 500:
            hidden_dims = [256, 128, 64]
        elif main_branch_dim > 200:
            hidden_dims = [128, 64]
        else:
            hidden_dims = [64, 32]

        # BRANCHE PRINCIPALE (Student)
        main_layers = []
        prev_dim = main_branch_dim

        for hidden_dim in hidden_dims:
            main_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim

        self.main_branch = nn.Sequential(*main_layers)

        self.main_attention = nn.Sequential(
            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),
            nn.ReLU(),
            nn.Linear(hidden_dims[-1] // 2, 1),
            nn.Sigmoid()
        )

        self.main_classifier = nn.Sequential(
            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(hidden_dims[-1] // 2, 2)
        )

        # BRANCHE TEACHER
        teacher_layers = []
        prev_dim = teacher_branch_dim

        for hidden_dim in hidden_dims:
            teacher_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim

        self.teacher_branch = nn.Sequential(*teacher_layers)

        self.teacher_attention = nn.Sequential(
            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),
            nn.ReLU(),
            nn.Linear(hidden_dims[-1] // 2, 1),
            nn.Sigmoid()
        )

        self.teacher_classifier = nn.Sequential(
            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(hidden_dims[-1] // 2, 2)
        )

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.BatchNorm1d):
            nn.init.constant_(module.weight, 1)
            nn.init.constant_(module.bias, 0)

    def forward(self, code_emb, commit_emb, metrics, syntax, use_teacher=False):
        # Construction dynamique de l'input selon les modalit√©s
        main_inputs = []
        if 'code' in self.main_modalities:
            main_inputs.append(code_emb)
        if 'commit' in self.main_modalities:
            main_inputs.append(commit_emb)
        if 'metrics' in self.main_modalities:
            main_inputs.append(metrics)
        if 'syntax' in self.main_modalities:
            main_inputs.append(syntax)

        main_input = torch.cat(main_inputs, dim=1)
        main_features = self.main_branch(main_input)
        main_attention_weights = self.main_attention(main_features)
        main_features_attended = main_features * main_attention_weights
        main_output = self.main_classifier(main_features_attended)

        if not use_teacher or commit_emb is None:
            return main_output

        teacher_inputs = []
        if 'code' in self.teacher_modalities:
            teacher_inputs.append(code_emb)
        if 'commit' in self.teacher_modalities:
            teacher_inputs.append(commit_emb)
        if 'metrics' in self.teacher_modalities:
            teacher_inputs.append(metrics)
        if 'syntax' in self.teacher_modalities:
            teacher_inputs.append(syntax)

        teacher_input = torch.cat(teacher_inputs, dim=1)
        teacher_features = self.teacher_branch(teacher_input)
        teacher_attention_weights = self.teacher_attention(teacher_features)
        teacher_features_attended = teacher_features * teacher_attention_weights
        teacher_output = self.teacher_classifier(teacher_features_attended)

        return main_output, teacher_output, main_features_attended, teacher_features_attended


# ================================
# LOSS DE DISTILLATION
# ================================

class DistillationLoss(nn.Module):
    def __init__(self, alpha=0.4, beta=0.4, temperature=3.0,
                 class_weights=None, feature_dim=128,
                 main_feature_dim=None, teacher_feature_dim=None):
        super(DistillationLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = 1 - alpha - beta
        self.temperature = temperature
        self.feature_dim = feature_dim

        self.ce_loss = nn.CrossEntropyLoss(
            weight=class_weights,
            label_smoothing=0.1
        )

        if main_feature_dim is not None and teacher_feature_dim is not None:
            self.main_projector = nn.Linear(main_feature_dim, feature_dim)
            self.teacher_projector = nn.Linear(teacher_feature_dim, feature_dim)
        else:
            self.main_projector = None
            self.teacher_projector = None

    def _init_projectors(self, main_dim, teacher_dim, device):
        if self.main_projector is None:
            self.main_projector = nn.Linear(main_dim, self.feature_dim).to(device)
            self.teacher_projector = nn.Linear(teacher_dim, self.feature_dim).to(device)
            self.add_module('main_projector', self.main_projector)
            self.add_module('teacher_projector', self.teacher_projector)

    def cosine_similarity_loss(self, feat1, feat2):
        if self.main_projector is None:
            device = feat1.device
            self._init_projectors(feat1.size(1), feat2.size(1), device)

        feat1_proj = self.main_projector(feat1)
        feat2_proj = self.teacher_projector(feat2)

        feat1_norm = F.normalize(feat1_proj, p=2, dim=1)
        feat2_norm = F.normalize(feat2_proj, p=2, dim=1)

        cos_sim = (feat1_norm * feat2_norm).sum(dim=1).mean()

        return 1 - cos_sim

    def forward(self, main_output, teacher_output, main_features, teacher_features, labels):
        loss_main = self.ce_loss(main_output, labels)
        loss_teacher = self.ce_loss(teacher_output, labels)

        soft_main = F.log_softmax(main_output / self.temperature, dim=1)
        soft_teacher = F.softmax(teacher_output.detach() / self.temperature, dim=1)
        loss_distillation = F.kl_div(
            soft_main, soft_teacher,
            reduction='batchmean'
        ) * (self.temperature ** 2)

        loss_features = self.cosine_similarity_loss(
            main_features,
            teacher_features.detach()
        )

        total_loss = (
            self.alpha * loss_main +
            self.beta * loss_distillation +
            self.gamma * loss_teacher +
            0.1 * loss_features
        )

        loss_dict = {
            'loss_main': loss_main.item(),
            'loss_teacher': loss_teacher.item(),
            'loss_distillation': loss_distillation.item(),
            'loss_features': loss_features.item(),
            'loss_total': total_loss.item()
        }

        return total_loss, loss_dict


# ================================
# DATASETS
# ================================

class VulnerabilityDataset(Dataset):
    def __init__(self, code_emb, commit_emb, metrics, syntax, labels, modalities_to_use=None):
        self.code_emb = torch.FloatTensor(code_emb)
        self.commit_emb = torch.FloatTensor(commit_emb)
        self.metrics = torch.FloatTensor(metrics)
        self.syntax = torch.FloatTensor(syntax)
        self.labels = torch.LongTensor(labels)
        self.modalities_to_use = modalities_to_use or ['code', 'metrics', 'syntax']

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        if len(self.modalities_to_use) == 1:
            if 'code' in self.modalities_to_use:
                return self.code_emb[idx], self.labels[idx]
            elif 'commit' in self.modalities_to_use:
                return self.commit_emb[idx], self.labels[idx]
            elif 'metrics' in self.modalities_to_use:
                return self.metrics[idx], self.labels[idx]
            elif 'syntax' in self.modalities_to_use:
                return self.syntax[idx], self.labels[idx]
        else:
            inputs = []
            for modality in self.modalities_to_use:
                if modality == 'code':
                    inputs.append(self.code_emb[idx])
                elif modality == 'commit':
                    inputs.append(self.commit_emb[idx])
                elif modality == 'metrics':
                    inputs.append(self.metrics[idx])
                elif modality == 'syntax':
                    inputs.append(self.syntax[idx])
            return torch.cat(inputs, dim=0), self.labels[idx]


class DualBranchDataset(Dataset):
    def __init__(self, code_emb, commit_emb, metrics, syntax, labels, is_training=True):
        self.code_emb = torch.FloatTensor(code_emb)
        self.commit_emb = torch.FloatTensor(commit_emb)
        self.metrics = torch.FloatTensor(metrics)
        self.syntax = torch.FloatTensor(syntax)
        self.labels = torch.LongTensor(labels)
        self.is_training = is_training

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return (self.code_emb[idx], self.commit_emb[idx],
                self.metrics[idx], self.syntax[idx], self.labels[idx])


# ================================
# PIPELINE D'ANALYSE 
# ================================

class FairMultimodalAnalysisPipeline:
    def __init__(self, train_csv_path=None, val_csv_path=None, test_csv_path=None,
                 single_csv_path=None, train_size=0.7, val_size=0.15, random_state=42):
        """
        Initialise le pipeline avec soit 3 fichiers CSV pr√©-splitt√©s, soit un CSV unique

        Args:
            train_csv_path: Chemin vers le CSV d'entra√Ænement (mode 3 fichiers)
            val_csv_path: Chemin vers le CSV de validation (mode 3 fichiers)
            test_csv_path: Chemin vers le CSV de test (mode 3 fichiers)
            single_csv_path: Chemin vers un CSV unique (mode 1 fichier)
            train_size: Proportion de train si single_csv_path est utilis√© (d√©faut: 0.7)
            val_size: Proportion de validation si single_csv_path est utilis√© (d√©faut: 0.15)
            random_state: Seed pour la reproductibilit√©
        """
        print("=== CHARGEMENT DES DONN√âES ===")

        if single_csv_path is not None:
            # Mode: un seul fichier CSV, on fait le split
            print(f" Mode: Fichier unique avec split automatique")
            print(f"   Fichier: {single_csv_path}")
            print(f"   Split: Train={train_size:.0%}, Val={val_size:.0%}, Test={1-train_size-val_size:.0%}")

            df = pd.read_csv(single_csv_path)

            # Split stratifi√© train / temp
            train_df, temp_df = train_test_split(
                df, train_size=train_size, random_state=random_state,
                stratify=df['target']
            )

            # Split stratifi√© val / test
            val_ratio = val_size / (1 - train_size)
            val_df, test_df = train_test_split(
                temp_df, train_size=val_ratio, random_state=random_state,
                stratify=temp_df['target']
            )

            self.train_df = train_df.reset_index(drop=True)
            self.val_df = val_df.reset_index(drop=True)
            self.test_df = test_df.reset_index(drop=True)

        else:
            # Mode: 3 fichiers pr√©-splitt√©s
            print(f" Mode: 3 fichiers pr√©-splitt√©s")
            self.train_df = pd.read_csv(train_csv_path)
            self.val_df = pd.read_csv(val_csv_path)
            self.test_df = pd.read_csv(test_csv_path)

        print(f" Train: {len(self.train_df)} √©chantillons")
        print(f" Val:   {len(self.val_df)} √©chantillons")
        print(f" Test:  {len(self.test_df)} √©chantillons")

        self.val_results = {}
        self.test_results = {}
        self.training_histories = {}
        self.prepare_data()

    def prepare_data(self):
        """Pr√©pare les donn√©es depuis les 3 fichiers CSV"""
        print("\n=== PR√âPARATION DES DONN√âES ===")

        # Extraction des colonnes
        code_emb_cols = [col for col in self.train_df.columns if col.startswith('code_emb_')]
        commit_emb_cols = [col for col in self.train_df.columns if col.startswith('msg_emb_')]
        metrics_cols = [col for col in self.train_df.columns if col.startswith('metric_')]
        syntax_cols = [col for col in self.train_df.columns if col.startswith('syntax_')]

        print(f"\n Structure des features:")
        print(f"  - Code embeddings:   {len(code_emb_cols)} dimensions")
        print(f"  - Commit embeddings: {len(commit_emb_cols)} dimensions")
        print(f"  - Metrics:           {len(metrics_cols)} features")
        print(f"  - Syntax:            {len(syntax_cols)} features")

        # TRAIN
        self.train_code_embeddings = self.train_df[code_emb_cols].values
        self.train_commit_embeddings = self.train_df[commit_emb_cols].values
        self.train_code_metrics = self.train_df[metrics_cols].values
        self.train_syntax_features = self.train_df[syntax_cols].values
        self.train_labels = self.train_df['target'].values

        # VAL
        self.val_code_embeddings = self.val_df[code_emb_cols].values
        self.val_commit_embeddings = self.val_df[commit_emb_cols].values
        self.val_code_metrics = self.val_df[metrics_cols].values
        self.val_syntax_features = self.val_df[syntax_cols].values
        self.val_labels = self.val_df['target'].values

        # TEST
        self.test_code_embeddings = self.test_df[code_emb_cols].values
        self.test_commit_embeddings = self.test_df[commit_emb_cols].values
        self.test_code_metrics = self.test_df[metrics_cols].values
        self.test_syntax_features = self.test_df[syntax_cols].values
        self.test_labels = self.test_df['target'].values

        # Features sans commit (pour baselines et unimodaux)
        self.train_features_without_commit = np.concatenate([
            self.train_code_embeddings, self.train_code_metrics, self.train_syntax_features
        ], axis=1)

        self.val_features_without_commit = np.concatenate([
            self.val_code_embeddings, self.val_code_metrics, self.val_syntax_features
        ], axis=1)

        self.test_features_without_commit = np.concatenate([
            self.test_code_embeddings, self.test_code_metrics, self.test_syntax_features
        ], axis=1)

        # Normalisation (fit sur train, transform sur val et test)
        self.scaler = StandardScaler()
        self.train_features_scaled = self.scaler.fit_transform(self.train_features_without_commit)
        self.val_features_scaled = self.scaler.transform(self.val_features_without_commit)
        self.test_features_scaled = self.scaler.transform(self.test_features_without_commit)

        # Distribution des labels
        train_dist = np.bincount(self.train_labels)
        val_dist = np.bincount(self.val_labels)
        test_dist = np.bincount(self.test_labels)

        print(f"\n Distribution des classes:")
        print(f"  Train: Classe 0={train_dist[0]} ({train_dist[0]/len(self.train_labels)*100:.1f}%), "
              f"Classe 1={train_dist[1]} ({train_dist[1]/len(self.train_labels)*100:.1f}%)")
        print(f"  Val:   Classe 0={val_dist[0]} ({val_dist[0]/len(self.val_labels)*100:.1f}%), "
              f"Classe 1={val_dist[1]} ({val_dist[1]/len(self.val_labels)*100:.1f}%)")
        print(f"  Test:  Classe 0={test_dist[0]} ({test_dist[0]/len(self.test_labels)*100:.1f}%), "
              f"Classe 1={test_dist[1]} ({test_dist[1]/len(self.test_labels)*100:.1f}%)")

    def mcnemar_test(self, model1_preds, model2_preds, labels):
        """Test de McNemar pour comparer deux mod√®les"""
        # Cr√©er la table de contingence
        both_correct = np.sum((model1_preds == labels) & (model2_preds == labels))
        model1_correct_only = np.sum((model1_preds == labels) & (model2_preds != labels))
        model2_correct_only = np.sum((model1_preds != labels) & (model2_preds == labels))
        both_wrong = np.sum((model1_preds != labels) & (model2_preds != labels))

        # Table de contingence pour McNemar
        contingency_table = [[both_correct, model1_correct_only],
                            [model2_correct_only, both_wrong]]

        # Test de McNemar (avec correction de continuit√©)
        result = mcnemar(contingency_table, exact=False, correction=True)

        return {
            'statistic': result.statistic,
            'p_value': result.pvalue,
            'both_correct': both_correct,
            'model1_only': model1_correct_only,
            'model2_only': model2_correct_only,
            'both_wrong': both_wrong
        }

    def statistical_comparison(self, dataset='test'):
        """Comparaison statistique des mod√®les avec test de McNemar"""
        print(f"\n{'='*80}")
        print(f" COMPARAISON STATISTIQUE DES MOD√àLES ({dataset.upper()})")
        print(f"{'='*80}")

        results = self.test_results if dataset == 'test' else self.val_results
        labels = self.test_labels if dataset == 'test' else self.val_labels

        if len(results) < 2:
            print("  Pas assez de mod√®les pour la comparaison")
            return

        # S√©lectionner les mod√®les √† comparer
        model_names = list(results.keys())

        # Trouver le meilleur mod√®le
        best_model = max(model_names, key=lambda x: results[x]['macro_f1'])

        print(f"\n Mod√®le de r√©f√©rence: {best_model} (F1={results[best_model]['macro_f1']:.4f})")
        print(f"\n{'Mod√®le Compar√©':<45} {'Statistic':<12} {'p-value':<12} {'Significatif':<12}")
        print("-" * 90)

        comparisons = []

        for model_name in model_names:
            if model_name == best_model:
                continue

            test_result = self.mcnemar_test(
                results[best_model]['y_pred'],
                results[model_name]['y_pred'],
                labels
            )

            is_significant = "Oui (p<0.05)" if test_result['p_value'] < 0.05 else "Non"

            print(f"{model_name:<45} {test_result['statistic']:<12.4f} "
                  f"{test_result['p_value']:<12.4f} {is_significant:<12}")

            comparisons.append({
                'model': model_name,
                'f1': results[model_name]['macro_f1'],
                'statistic': test_result['statistic'],
                'p_value': test_result['p_value'],
                'significant': test_result['p_value'] < 0.05
            })

        # Afficher les d√©tails pour les comparaisons significatives
        significant_comps = [c for c in comparisons if c['significant']]
        if significant_comps:
            print(f"\n Diff√©rences significatives d√©tect√©es ({len(significant_comps)} mod√®les)")
            print(f"   ‚Üí Le mod√®le de r√©f√©rence est statistiquement meilleur que ces mod√®les (p<0.05)")
        else:
            print(f"\nÔ∏è  Aucune diff√©rence significative d√©tect√©e")
            print(f"   ‚Üí Les diff√©rences de performance pourraient √™tre dues au hasard")

    def train_pytorch_model(self, model, train_loader, val_loader, model_name, epochs=100):
        """Entra√Æne un mod√®le PyTorch avec validation"""
        unique, counts = np.unique(self.train_labels, return_counts=True)
        class_weights = len(self.train_labels) / (len(unique) * counts)
        class_weights = np.clip(class_weights, 0.5, 2.0)
        weights = torch.FloatTensor([class_weights[0], class_weights[1]])

        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3, betas=(0.9, 0.999), eps=1e-8)
        criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)
        scheduler = optim.lr_scheduler.OneCycleLR(
            optimizer, max_lr=0.001, epochs=epochs, steps_per_epoch=len(train_loader),
            pct_start=0.1, anneal_strategy='cos'
        )

        train_losses, train_f1s = [], []
        val_losses, val_f1s = [], []
        best_f1 = -1
        patience_counter = 0
        best_epoch = 0

        for epoch in range(epochs):
            model.train()
            epoch_loss = 0
            all_preds, all_labels = [], []

            for batch_idx, batch in enumerate(train_loader):
                features, labels = batch
                optimizer.zero_grad()

                outputs = model(features)
                loss = criterion(outputs, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()

                epoch_loss += loss.item()
                with torch.no_grad():
                    _, predicted = torch.max(outputs, 1)
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())

            train_loss = epoch_loss / len(train_loader)
            train_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)

            model.eval()
            val_loss = 0
            val_preds, val_labels = [], []

            with torch.no_grad():
                for batch in val_loader:
                    features, labels = batch
                    outputs = model(features)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item()

                    _, predicted = torch.max(outputs, 1)
                    val_preds.extend(predicted.cpu().numpy())
                    val_labels.extend(labels.cpu().numpy())

            val_loss = val_loss / len(val_loader)
            val_f1 = f1_score(val_labels, val_preds, average='macro', zero_division=0)

            train_losses.append(train_loss)
            train_f1s.append(train_f1)
            val_losses.append(val_loss)
            val_f1s.append(val_f1)

            if (epoch + 1) % 10 == 0:
                print(f'{model_name} - Epoch {epoch+1}/{epochs}: '
                      f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}')

            if val_f1 > best_f1 + 0.001:
                best_f1 = val_f1
                best_epoch = epoch
                patience_counter = 0
                torch.save(model.state_dict(), f'best_{model_name.lower().replace(" ", "_").replace("+", "")}.pth')
            else:
                patience_counter += 1

            if patience_counter >= 20:
                print(f"Early stopping at epoch {epoch+1} (best: {best_epoch+1})")
                break

        try:
            model.load_state_dict(torch.load(f'best_{model_name.lower().replace(" ", "_").replace("+", "")}.pth'))
            print(f" Best model loaded (epoch {best_epoch+1}, Val F1: {best_f1:.4f})")
        except Exception as e:
            print(f"  Could not load best model: {e}")

        self.training_histories[model_name] = {
            'train_losses': train_losses,
            'train_f1s': train_f1s,
            'val_losses': val_losses,
            'val_f1s': val_f1s,
            'best_epoch': best_epoch,
            'best_f1': best_f1
        }

        return model

    def train_dual_branch_model(self, epochs=100, main_modalities=None, teacher_modalities=None):
        """Entra√Æne le mod√®le dual branch avec pr√©-entra√Ænement"""
        if main_modalities is None:
            main_modalities = ['code', 'metrics', 'syntax']
        if teacher_modalities is None:
            teacher_modalities = ['code', 'commit', 'metrics', 'syntax']

        print(f"\n  Entra√Ænement avec modalit√©s:")
        print(f"   Main (Student): {main_modalities}")
        print(f"   Teacher: {teacher_modalities}")

        model, teacher_f1, student_f1 = self.train_dual_branch_with_pretraining(
            pretrain_epochs=50,
            distill_epochs=epochs,
            main_modalities=main_modalities,
            teacher_modalities=teacher_modalities
        )
        return model

    def train_dual_branch_with_pretraining(self, pretrain_epochs=50, distill_epochs=100,
                                           main_modalities=None, teacher_modalities=None):
        """Entra√Æne le mod√®le dual branch en 2 phases"""
        if main_modalities is None:
            main_modalities = ['code', 'metrics', 'syntax']
        if teacher_modalities is None:
            teacher_modalities = ['code', 'commit', 'metrics', 'syntax']

        model_name_suffix = f"_{'_'.join(sorted(main_modalities))}"

        print("\n" + "="*80)
        print(" ENTRA√éNEMENT DU MOD√àLE √Ä DOUBLE BRANCHE AVEC PR√â-ENTRA√éNEMENT")
        print("="*80)
        print(f"Main modalities: {main_modalities}")
        print(f"Teacher modalities: {teacher_modalities}")
        print("="*80)

        # Pr√©paration des datasets
        train_dataset = DualBranchDataset(
            self.train_code_embeddings,
            self.train_commit_embeddings,
            self.train_code_metrics,
            self.train_syntax_features,
            self.train_labels,
            is_training=True
        )

        val_dataset = DualBranchDataset(
            self.val_code_embeddings,
            self.val_commit_embeddings,
            self.val_code_metrics,
            self.val_syntax_features,
            self.val_labels,
            is_training=False
        )

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

        # Initialisation du mod√®le
        model = DualBranchModel(
            code_dim=self.train_code_embeddings.shape[1],
            commit_dim=self.train_commit_embeddings.shape[1],
            metrics_dim=self.train_code_metrics.shape[1],
            syntax_dim=self.train_syntax_features.shape[1],
            dropout_rate=0.3,
            main_modalities=main_modalities,
            teacher_modalities=teacher_modalities
        )

        # Calcul des poids de classes
        unique, counts = np.unique(self.train_labels, return_counts=True)
        if len(counts) > 1:
            class_weights = len(self.train_labels) / (len(unique) * counts)
            class_weights = np.clip(class_weights, 0.5, 2.0)
            weights = torch.FloatTensor([class_weights[0], class_weights[1]])
        else:
            weights = None

        # Calculer les dimensions des features
        main_branch_dim = sum([
            self.train_code_embeddings.shape[1] if 'code' in main_modalities else 0,
            self.train_commit_embeddings.shape[1] if 'commit' in main_modalities else 0,
            self.train_code_metrics.shape[1] if 'metrics' in main_modalities else 0,
            self.train_syntax_features.shape[1] if 'syntax' in main_modalities else 0
        ])

        if main_branch_dim > 1000:
            hidden_dims = [512, 256, 128]
        elif main_branch_dim > 500:
            hidden_dims = [256, 128, 64]
        elif main_branch_dim > 200:
            hidden_dims = [128, 64]
        else:
            hidden_dims = [64, 32]

        # ========================================================================
        # PHASE 1: PR√â-ENTRA√éNEMENT DU TEACHER
        # ========================================================================
        print("\n" + "="*80)
        print(" PHASE 1: PR√â-ENTRA√éNEMENT DU TEACHER (avec commits)")
        print("="*80)

        teacher_criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)
        teacher_optimizer = optim.AdamW(
            list(model.teacher_branch.parameters()) +
            list(model.teacher_attention.parameters()) +
            list(model.teacher_classifier.parameters()),
            lr=0.001,
            weight_decay=1e-3
        )

        teacher_scheduler = optim.lr_scheduler.OneCycleLR(
            teacher_optimizer,
            max_lr=0.001,
            epochs=pretrain_epochs,
            steps_per_epoch=len(train_loader),
            pct_start=0.1,
            anneal_strategy='cos'
        )

        best_teacher_f1 = -1
        patience_counter = 0

        for epoch in range(pretrain_epochs):
            model.train()
            epoch_loss = 0
            all_preds, all_labels = [], []

            for batch in train_loader:
                code, commit, metrics, syntax, labels = batch
                teacher_optimizer.zero_grad()

                # Construction dynamique de l'input teacher
                teacher_inputs = []
                if 'code' in teacher_modalities:
                    teacher_inputs.append(code)
                if 'commit' in teacher_modalities:
                    teacher_inputs.append(commit)
                if 'metrics' in teacher_modalities:
                    teacher_inputs.append(metrics)
                if 'syntax' in teacher_modalities:
                    teacher_inputs.append(syntax)

                teacher_input = torch.cat(teacher_inputs, dim=1)
                teacher_features = model.teacher_branch(teacher_input)
                teacher_attention_weights = model.teacher_attention(teacher_features)
                teacher_features_attended = teacher_features * teacher_attention_weights
                teacher_output = model.teacher_classifier(teacher_features_attended)

                loss = teacher_criterion(teacher_output, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    list(model.teacher_branch.parameters()) +
                    list(model.teacher_attention.parameters()) +
                    list(model.teacher_classifier.parameters()),
                    max_norm=1.0
                )
                teacher_optimizer.step()
                teacher_scheduler.step()

                epoch_loss += loss.item()

                with torch.no_grad():
                    _, predicted = torch.max(teacher_output, 1)
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())

            train_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)

            # Validation du teacher
            model.eval()
            val_loss = 0
            val_preds, val_labels = [], []

            with torch.no_grad():
                for batch in val_loader:
                    code, commit, metrics, syntax, labels = batch

                    # Construction dynamique de l'input teacher
                    teacher_inputs = []
                    if 'code' in teacher_modalities:
                        teacher_inputs.append(code)
                    if 'commit' in teacher_modalities:
                        teacher_inputs.append(commit)
                    if 'metrics' in teacher_modalities:
                        teacher_inputs.append(metrics)
                    if 'syntax' in teacher_modalities:
                        teacher_inputs.append(syntax)

                    teacher_input = torch.cat(teacher_inputs, dim=1)
                    teacher_features = model.teacher_branch(teacher_input)
                    teacher_attention_weights = model.teacher_attention(teacher_features)
                    teacher_features_attended = teacher_features * teacher_attention_weights
                    teacher_output = model.teacher_classifier(teacher_features_attended)

                    loss = teacher_criterion(teacher_output, labels)
                    val_loss += loss.item()

                    _, predicted = torch.max(teacher_output, 1)
                    val_preds.extend(predicted.cpu().numpy())
                    val_labels.extend(labels.cpu().numpy())

            val_f1 = f1_score(val_labels, val_preds, average='macro', zero_division=0)

            if (epoch + 1) % 10 == 0:
                print(f'[Teacher Pretrain] Epoch {epoch+1}/{pretrain_epochs}: '
                      f'Train F1={train_f1:.4f}, Val F1={val_f1:.4f}')

            if val_f1 > best_teacher_f1 + 0.001:
                best_teacher_f1 = val_f1
                patience_counter = 0
                torch.save({
                    'teacher_branch': model.teacher_branch.state_dict(),
                    'teacher_attention': model.teacher_attention.state_dict(),
                    'teacher_classifier': model.teacher_classifier.state_dict(),
                }, f'best_teacher_pretrained{model_name_suffix}.pth')
            else:
                patience_counter += 1

            if patience_counter >= 15:
                print(f"Early stopping at epoch {epoch+1}")
                break

        # Charger le meilleur teacher
        teacher_checkpoint = torch.load(f'best_teacher_pretrained{model_name_suffix}.pth')
        model.teacher_branch.load_state_dict(teacher_checkpoint['teacher_branch'])
        model.teacher_attention.load_state_dict(teacher_checkpoint['teacher_attention'])
        model.teacher_classifier.load_state_dict(teacher_checkpoint['teacher_classifier'])

        print(f"\n PHASE 1 TERMIN√âE: Teacher pr√©-entra√Æn√© (Val F1: {best_teacher_f1:.4f})")

        # FIGER le teacher
        for param in model.teacher_branch.parameters():
            param.requires_grad = False
        for param in model.teacher_attention.parameters():
            param.requires_grad = False
        for param in model.teacher_classifier.parameters():
            param.requires_grad = False

        print(" Teacher fig√© (frozen) pour la distillation")

        # ========================================================================
        # PHASE 2: DISTILLATION
        # ========================================================================
        print("\n" + "="*80)
        print("üéì PHASE 2: DISTILLATION (teacher ‚Üí student)")
        print("="*80)

        distill_criterion = DistillationLoss(
            alpha=0.4,
            beta=0.4,
            temperature=3.0,
            class_weights=weights,
            feature_dim=128,
            main_feature_dim=hidden_dims[-1],
            teacher_feature_dim=hidden_dims[-1]
        )

        distill_optimizer = optim.AdamW(
            list(model.main_branch.parameters()) +
            list(model.main_attention.parameters()) +
            list(model.main_classifier.parameters()) +
            list(distill_criterion.parameters()),
            lr=0.001,
            weight_decay=1e-3
        )

        distill_scheduler = optim.lr_scheduler.OneCycleLR(
            distill_optimizer,
            max_lr=0.001,
            epochs=distill_epochs,
            steps_per_epoch=len(train_loader),
            pct_start=0.1,
            anneal_strategy='cos'
        )

        best_student_f1 = -1
        patience_counter = 0

        for epoch in range(distill_epochs):
            model.train()
            epoch_loss = 0
            all_preds, all_labels = [], []

            for batch in train_loader:
                code, commit, metrics, syntax, labels = batch
                distill_optimizer.zero_grad()

                main_output, teacher_output, main_features, teacher_features = model(
                    code, commit, metrics, syntax, use_teacher=True
                )

                loss, loss_dict = distill_criterion(
                    main_output, teacher_output,
                    main_features, teacher_features,
                    labels
                )

                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    list(model.main_branch.parameters()) +
                    list(model.main_attention.parameters()) +
                    list(model.main_classifier.parameters()),
                    max_norm=1.0
                )
                distill_optimizer.step()
                distill_scheduler.step()

                epoch_loss += loss.item()

                with torch.no_grad():
                    _, predicted = torch.max(main_output, 1)
                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())

            train_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)

            # Validation du student (SANS commits)
            model.eval()
            val_preds, val_labels = [], []

            with torch.no_grad():
                for batch in val_loader:
                    code, commit, metrics, syntax, labels = batch
                    main_output = model(code, None, metrics, syntax, use_teacher=False)

                    _, predicted = torch.max(main_output, 1)
                    val_preds.extend(predicted.cpu().numpy())
                    val_labels.extend(labels.cpu().numpy())

            val_f1 = f1_score(val_labels, val_preds, average='macro', zero_division=0)

            if (epoch + 1) % 10 == 0:
                print(f'[Distillation] Epoch {epoch+1}/{distill_epochs}: '
                      f'Train F1={train_f1:.4f}, Val F1={val_f1:.4f}')

            if val_f1 > best_student_f1 + 0.001:
                best_student_f1 = val_f1
                patience_counter = 0
                torch.save(model.state_dict(), f'best_dual_branch_distilled{model_name_suffix}.pth')
            else:
                patience_counter += 1

            if patience_counter >= 20:
                print(f"Early stopping at epoch {epoch+1}")
                break

        # Charger le meilleur mod√®le
        model.load_state_dict(torch.load(f'best_dual_branch_distilled{model_name_suffix}.pth'))

        print(f"\n PHASE 2 TERMIN√âE: Student distill√© (Val F1: {best_student_f1:.4f})")
        print(f" Gain vs teacher seul: {best_student_f1 - best_teacher_f1:+.4f}")

        print("\n" + "="*80)
        print(" ENTRA√éNEMENT TERMIN√â")
        print("="*80)
        print(f"Teacher (avec commits): {best_teacher_f1:.4f}")
        print(f"Student (sans commits): {best_student_f1:.4f}")
        print("="*80 + "\n")

        return model, best_teacher_f1, best_student_f1

    def evaluate_model(self, model, loader, model_name, on_test=False):
        """√âvalue un mod√®le sur validation OU test"""
        model.eval()
        all_preds, all_labels, all_probs = [], [], []

        with torch.no_grad():
            for batch in loader:
                features, labels = batch
                outputs = model(features)

                probs = torch.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs, 1)

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs[:, 1].cpu().numpy())

        return self.calculate_metrics(all_labels, all_preds, all_probs, model_name, on_test)

    def evaluate_dual_branch(self, model, loader, model_name, on_test=False):
        """√âvalue le mod√®le dual branch"""
        model.eval()
        all_preds, all_labels, all_probs = [], [], []

        with torch.no_grad():
            for batch in loader:
                code, commit, metrics, syntax, labels = batch
                output = model(code, None, metrics, syntax, use_teacher=False)

                probs = torch.softmax(output, dim=1)
                _, predicted = torch.max(output, 1)

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs[:, 1].cpu().numpy())

        return self.calculate_metrics(all_labels, all_preds, all_probs, model_name, on_test)

    def calculate_metrics(self, y_true, y_pred, y_prob, model_name, on_test=False):
        """Calcule et sauvegarde les m√©triques"""
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        y_prob = np.array(y_prob)

        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, labels=[0, 1], zero_division=0
        )

        accuracy = np.sum(y_true == y_pred) / len(y_true)

        metrics = {
            'accuracy': accuracy,
            'precision_0': precision[0] if len(precision) > 0 else 0,
            'recall_0': recall[0] if len(recall) > 0 else 0,
            'f1_0': f1[0] if len(f1) > 0 else 0,
            'precision_1': precision[1] if len(precision) > 1 else 0,
            'recall_1': recall[1] if len(recall) > 1 else 0,
            'f1_1': f1[1] if len(f1) > 1 else 0,
            'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),
            'weighted_f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),
            'auc': roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.5,
            'y_true': y_true,
            'y_pred': y_pred,
            'y_prob': y_prob
        }

        if on_test:
            self.test_results[model_name] = metrics
            print(f" {model_name} - TEST: F1={metrics['macro_f1']:.4f}, AUC={metrics['auc']:.4f}")
        else:
            self.val_results[model_name] = metrics
            print(f" {model_name} - VAL: F1={metrics['macro_f1']:.4f}, AUC={metrics['auc']:.4f}")

        return metrics

    def train_baseline_models(self):
        """Entra√Æne les mod√®les baseline"""
        print("\n=== BASELINES (Code + Metrics + Syntax) ===")

        baselines = {
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),
            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
            'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),
        }

        for name, model in baselines.items():
            print(f"\n Entra√Ænement de {name}...")
            model.fit(self.train_features_scaled, self.train_labels)

            y_val_pred = model.predict(self.val_features_scaled)
            y_val_prob = model.predict_proba(self.val_features_scaled)[:, 1]
            self.calculate_metrics(self.val_labels, y_val_pred, y_val_prob, name, on_test=False)

            y_test_pred = model.predict(self.test_features_scaled)
            y_test_prob = model.predict_proba(self.test_features_scaled)[:, 1]
            self.calculate_metrics(self.test_labels, y_test_pred, y_test_prob, name, on_test=True)

    def train_combination_model(self, mods, name):
        """Entra√Æne un mod√®le pour une combinaison de modalit√©s"""
        if 'commit' in mods:
            print(f"\n  '{name}' inclut commit - IGNOR√â")
            return

        input_dim = sum([
            self.train_code_embeddings.shape[1] if 'code' in mods else 0,
            self.train_code_metrics.shape[1] if 'metrics' in mods else 0,
            self.train_syntax_features.shape[1] if 'syntax' in mods else 0
        ])

        train_dataset = VulnerabilityDataset(
            self.train_code_embeddings,
            self.train_commit_embeddings,
            self.train_code_metrics,
            self.train_syntax_features,
            self.train_labels,
            modalities_to_use=mods
        )
        val_dataset = VulnerabilityDataset(
            self.val_code_embeddings,
            self.val_commit_embeddings,
            self.val_code_metrics,
            self.val_syntax_features,
            self.val_labels,
            modalities_to_use=mods
        )
        test_dataset = VulnerabilityDataset(
            self.test_code_embeddings,
            self.test_commit_embeddings,
            self.test_code_metrics,
            self.test_syntax_features,
            self.test_labels,
            modalities_to_use=mods
        )

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

        model = UniModalModel(input_dim=input_dim, dropout_rate=0.3, use_attention=True)
        model = self.train_pytorch_model(model, train_loader, val_loader, name, epochs=100)

        self.evaluate_model(model, val_loader, name, on_test=False)
        self.evaluate_model(model, test_loader, name, on_test=True)

    def run_fair_analysis(self):
        """Lance l'analyse compl√®te"""
        print("="*80)
        print(" ANALYSE MULTI-MODALE FAIR - DATASET DEVIGN")
        print("="*80)
        print(" PROTOCOLE:")
        print("  1. Entra√Ænement sur TRAIN")
        print("  2. Early stopping sur VALIDATION")
        print("  3. ABLATION STUDY: comparaison sur VALIDATION")
        print("  4. √âVALUATION FINALE: tous mod√®les sur TEST")
        print("="*80)

        self.train_baseline_models()

        print("\n" + "="*80)
        print(" ABLATION STUDY - MOD√àLES UNI-MODAUX")
        print("="*80)

        for name, mods in [
            ('Code Only', ['code']),
            ('Metrics Only', ['metrics']),
            ('Syntax Only', ['syntax']),
        ]:
            print(f"\n {name}...")
            self.train_combination_model(mods, name)

        print("\n" + "="*80)
        print(" MOD√àLES BI-MODAUX")
        print("="*80)

        for name, mods in [
            ('Code + Metrics', ['code', 'metrics']),
            ('Code + Syntax', ['code', 'syntax']),
            ('Metrics + Syntax', ['metrics', 'syntax']),
        ]:
            print(f"\nüîß {name}...")
            self.train_combination_model(mods, name)

        print("\n" + "="*80)
        print(" MOD√àLE TRI-MODAL (R√âF√âRENCE)")
        print("="*80)
        self.train_combination_model(['code', 'metrics', 'syntax'], "Code + Metrics + Syntax")

        print("\n" + "="*80)
        print(" MOD√àLE √Ä DOUBLE BRANCHE - CONFIGURATION STANDARD")
        print("="*80)
        dual_model = self.train_dual_branch_model(
            epochs=100,
            main_modalities=['code', 'metrics', 'syntax'],
            teacher_modalities=['code', 'commit', 'metrics', 'syntax']
        )

        val_dataset = DualBranchDataset(
            self.val_code_embeddings,
            self.val_commit_embeddings,
            self.val_code_metrics,
            self.val_syntax_features,
            self.val_labels,
            is_training=False
        )
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        self.evaluate_dual_branch(dual_model, val_loader, 'Dual Branch (Code+Metrics+Syntax)', on_test=False)

        test_dataset = DualBranchDataset(
            self.test_code_embeddings,
            self.test_commit_embeddings,
            self.test_code_metrics,
            self.test_syntax_features,
            self.test_labels,
            is_training=False
        )
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
        self.evaluate_dual_branch(dual_model, test_loader, 'Dual Branch (Code+Metrics+Syntax)', on_test=True)

        # ========================================================================
        # NOUVELLES CONFIGURATIONS: Test de l'impact du commit sur chaque modalit√©
        # ========================================================================
        print("\n" + "="*80)
        print(" √âTUDE DE L'IMPACT DU COMMIT - CONFIGURATIONS ALTERNATIVES")
        print("="*80)

        commit_configs = [
            # Nom du mod√®le, modalit√©s main (student), modalit√©s teacher
            ('Dual Branch (Metrics)', ['metrics'], ['metrics', 'commit']),
            ('Dual Branch (Syntax)', ['syntax'], ['syntax', 'commit']),
            ('Dual Branch (Code)', ['code'], ['code', 'commit']),
            ('Dual Branch (Code+Metrics)', ['code', 'metrics'], ['code', 'metrics', 'commit']),
            ('Dual Branch (Metrics+Syntax)', ['metrics', 'syntax'], ['metrics', 'syntax', 'commit']),
            ('Dual Branch (Code+Syntax)', ['code', 'syntax'], ['code', 'syntax', 'commit']),
        ]

        for model_name, main_mods, teacher_mods in commit_configs:
            print(f"\n{'='*80}")
            print(f" Configuration: {model_name}")
            print(f"   Student: {main_mods}")
            print(f"   Teacher: {teacher_mods}")
            print(f"{'='*80}")

            dual_model_commit = self.train_dual_branch_model(
                epochs=100,
                main_modalities=main_mods,
                teacher_modalities=teacher_mods
            )

            # √âvaluation sur validation
            val_loader_commit = DataLoader(val_dataset, batch_size=64, shuffle=False)
            self.evaluate_dual_branch(dual_model_commit, val_loader_commit, model_name, on_test=False)

            # √âvaluation sur test
            test_loader_commit = DataLoader(test_dataset, batch_size=64, shuffle=False)
            self.evaluate_dual_branch(dual_model_commit, test_loader_commit, model_name, on_test=True)

        print("\n" + "="*80)
        print(" ANALYSE TERMIN√âE")
        print("="*80)

    def ablation_study_analysis(self):
        """Analyse de l'ablation study sur VALIDATION"""
        print("\n" + "="*80)
        print(" ABLATION STUDY - R√âSULTATS SUR VALIDATION")
        print("="*80)

        if not self.val_results:
            print("  Aucun r√©sultat de validation disponible")
            return

        baseline_models = []
        unimodal_models = []
        bimodal_models = []
        trimodal_std = None
        dual_branch_models = []

        for name in self.val_results.keys():
            if 'Dual Branch' in name:
                dual_branch_models.append(name)
            elif any(k in name for k in ['Random Forest', 'Gradient Boosting', 'Logistic Regression']):
                baseline_models.append(name)
            elif 'Only' in name:
                unimodal_models.append(name)
            elif name == "Code + Metrics + Syntax":
                trimodal_std = name
            elif '+' in name:
                bimodal_models.append(name)

        def display_category(models, title):
            if not models:
                return None

            print(f"\n{'='*80}")
            print(f"{title}")
            print(f"{'='*80}")

            sorted_models = sorted(models, key=lambda x: self.val_results[x]['macro_f1'], reverse=True)

            print(f"{'Rang':<4} {'Mod√®le':<40} {'F1 Macro':<10} {'AUC':<10}")
            print("-" * 80)

            for rank, name in enumerate(sorted_models, 1):
                metrics = self.val_results[name]
                print(f"{rank:<4} {name:<40} {metrics['macro_f1']:<10.4f} {metrics['auc']:<10.4f}")

            return sorted_models[0]

        best_baseline = display_category(baseline_models, "BASELINES")
        best_uni = display_category(unimodal_models, "UNI-MODAUX")
        best_bi = display_category(bimodal_models, "BI-MODAUX")
        best_dual = display_category(dual_branch_models, "DUAL BRANCH (avec Knowledge Distillation)")

        print(f"\n{'='*80}")
        print(" COMPARAISON DES CHAMPIONS (VALIDATION)")
        print(f"{'='*80}")

        champions = []
        if best_baseline:
            champions.append(('Baseline', best_baseline))
        if best_uni:
            champions.append(('Unimodal', best_uni))
        if best_bi:
            champions.append(('Bimodal', best_bi))
        if trimodal_std:
            champions.append(('Trimodal Std', trimodal_std))
        if best_dual:
            champions.append(('Best Dual Branch', best_dual))

        print(f"{'Cat√©gorie':<20} {'Mod√®le':<35} {'F1':<10} {'AUC':<10}")
        print("-" * 80)

        for cat, name in champions:
            m = self.val_results[name]
            print(f"{cat:<20} {name:<35} {m['macro_f1']:<10.4f} {m['auc']:<10.4f}")

        # Analyse de l'impact du commit
        if dual_branch_models:
            print(f"\n{'='*80}")
            print(" ANALYSE DE L'IMPACT DU COMMIT")
            print(f"{'='*80}")

            # Regrouper par configuration de base
            commit_impact = {}

            for dual_name in dual_branch_models:
                # Extraire la configuration (ex: "Dual Branch (Code+Metrics)" -> "Code+Metrics")
                if '(' in dual_name and ')' in dual_name:
                    config = dual_name.split('(')[1].split(')')[0]
                else:
                    config = "Code+Metrics+Syntax"

                f1 = self.val_results[dual_name]['macro_f1']

                # Trouver le mod√®le sans commit correspondant
                baseline_name = config.replace('+', ' + ')
                if baseline_name in self.val_results:
                    baseline_f1 = self.val_results[baseline_name]['macro_f1']
                    gain = f1 - baseline_f1
                    gain_pct = (gain / baseline_f1) * 100 if baseline_f1 > 0 else 0

                    commit_impact[config] = {
                        'dual_f1': f1,
                        'baseline_f1': baseline_f1,
                        'gain': gain,
                        'gain_pct': gain_pct
                    }

            if commit_impact:
                print(f"\n{'Configuration':<25} {'Sans Commit':<12} {'Avec Commit':<12} {'Gain':<12} {'Gain %':<10}")
                print("-" * 80)

                # Trier par gain d√©croissant
                sorted_impact = sorted(commit_impact.items(), key=lambda x: x[1]['gain'], reverse=True)

                for config, impact in sorted_impact:
                    print(f"{config:<25} {impact['baseline_f1']:<12.4f} {impact['dual_f1']:<12.4f} "
                          f"{impact['gain']:<12.4f} {impact['gain_pct']:<10.2f}%")

        if best_dual and trimodal_std:
            print(f"\n{'='*80}")
            print(" TEST CRITIQUE: Best Dual Branch vs Trimodal Standard")
            print(f"{'='*80}")

            dual_f1 = self.val_results[best_dual]['macro_f1']
            tri_f1 = self.val_results[trimodal_std]['macro_f1']

            diff = dual_f1 - tri_f1
            pct = (diff / tri_f1) * 100

            print(f"\n  Trimodal Standard: {tri_f1:.4f}")
            print(f"  Best Dual Branch:  {dual_f1:.4f}")
            print(f"  Diff√©rence:        {diff:+.4f} ({pct:+.2f}%)")

        print("\n" + "="*80)

    def final_test_evaluation(self):
        """√âvaluation finale sur TEST"""
        print("\n" + "="*80)
        print(" √âVALUATION FINALE - R√âSULTATS SUR TEST")
        print("="*80)

        if not self.test_results:
            print("  Aucun r√©sultat de test disponible")
            return

        model_names = list(self.test_results.keys())
        sorted_names = sorted(model_names, key=lambda x: self.test_results[x]['macro_f1'], reverse=True)

        print(f"\n{'Rang':<4} {'Mod√®le':<45} {'F1 Macro':<10} {'AUC':<10} {'Accuracy':<10}")
        print("-" * 90)

        for rank, name in enumerate(sorted_names, 1):
            m = self.test_results[name]
            print(f"{rank:<4} {name:<45} {m['macro_f1']:<10.4f} {m['auc']:<10.4f} {m['accuracy']:<10.4f}")

        best_name = sorted_names[0]
        best_m = self.test_results[best_name]

        print(f"\n{'='*80}")
        print(f" MEILLEUR MOD√àLE: {best_name}")
        print(f"{'='*80}")
        print(f" Performances d√©taill√©es:")
        print(f"   F1 Macro:    {best_m['macro_f1']:.4f}")
        print(f"   AUC:         {best_m['auc']:.4f}")
        print(f"   Accuracy:    {best_m['accuracy']:.4f}")

        # Analyse de l'impact du commit sur TEST
        print(f"\n{'='*80}")
        print(" ANALYSE DE L'IMPACT DU COMMIT - R√âSULTATS TEST")
        print(f"{'='*80}")

        commit_impact_test = {}

        for dual_name in [k for k in self.test_results.keys() if 'Dual Branch' in k]:
            if '(' in dual_name and ')' in dual_name:
                config = dual_name.split('(')[1].split(')')[0]
            else:
                config = "Code+Metrics+Syntax"

            f1 = self.test_results[dual_name]['macro_f1']
            baseline_name = config.replace('+', ' + ')

            if baseline_name in self.test_results:
                baseline_f1 = self.test_results[baseline_name]['macro_f1']
                gain = f1 - baseline_f1
                gain_pct = (gain / baseline_f1) * 100 if baseline_f1 > 0 else 0

                commit_impact_test[config] = {
                    'dual_f1': f1,
                    'baseline_f1': baseline_f1,
                    'gain': gain,
                    'gain_pct': gain_pct
                }

        if commit_impact_test:
            print(f"\n{'Configuration':<25} {'Sans Commit':<12} {'Avec Commit':<12} {'Gain':<12} {'Gain %':<10}")
            print("-" * 80)

            sorted_impact_test = sorted(commit_impact_test.items(), key=lambda x: x[1]['gain'], reverse=True)

            for config, impact in sorted_impact_test:
                print(f"{config:<25} {impact['baseline_f1']:<12.4f} {impact['dual_f1']:<12.4f} "
                      f"{impact['gain']:<12.4f} {impact['gain_pct']:<10.2f}%")

        # Appeler la comparaison statistique
        self.statistical_comparison(dataset='test')

        print(f"\n{'='*80}")
        print(" √âVALUATION TERMIN√âE")
        print(f"{'='*80}")

    def save_results(self, filename="devign_analysis_results.csv"):
        """Sauvegarde tous les r√©sultats"""
        print(f"\n Sauvegarde des r√©sultats dans {filename}...")

        results_data = []

        for model_name, metrics in self.val_results.items():
            results_data.append({
                'Model': model_name,
                'Dataset': 'Validation',
                'F1_Macro': metrics['macro_f1'],
                'AUC': metrics['auc'],
                'Accuracy': metrics['accuracy'],
                'Precision_0': metrics['precision_0'],
                'Recall_0': metrics['recall_0'],
                'F1_0': metrics['f1_0'],
                'Precision_1': metrics['precision_1'],
                'Recall_1': metrics['recall_1'],
                'F1_1': metrics['f1_1']
            })

        for model_name, metrics in self.test_results.items():
            results_data.append({
                'Model': model_name,
                'Dataset': 'Test',
                'F1_Macro': metrics['macro_f1'],
                'AUC': metrics['auc'],
                'Accuracy': metrics['accuracy'],
                'Precision_0': metrics['precision_0'],
                'Recall_0': metrics['recall_0'],
                'F1_0': metrics['f1_0'],
                'Precision_1': metrics['precision_1'],
                'Recall_1': metrics['recall_1'],
                'F1_1': metrics['f1_1']
            })

        results_df = pd.DataFrame(results_data)
        results_df.to_csv(filename, index=False)
        print(f" R√©sultats sauvegard√©s dans {filename}")


# ================================
# FONCTION PRINCIPALE
# ================================

def run_devign_multimodal_analysis(train_csv=None, val_csv=None, test_csv=None,
                                    single_csv=None, train_size=0.7, val_size=0.15):
    """
    Fonction principale pour l'analyse multimodale

    Args:
        train_csv, val_csv, test_csv: Chemins vers 3 fichiers pr√©-splitt√©s (mode 1)
        single_csv: Chemin vers un fichier unique √† splitter (mode 2)
        train_size: Proportion de train si single_csv (d√©faut: 0.7)
        val_size: Proportion de validation si single_csv (d√©faut: 0.15)
    """

    print(" " + "="*78 + " ")
    print("   ANALYSE MULTI-MODALE - DATASET DEVIGN")
    print(" " + "="*78 + " ")
    print("\n PROTOCOLE:")
    print("  1. Chargement des donn√©es (3 fichiers ou 1 fichier avec split)")
    print("  2. Entra√Ænement: TRAIN")
    print("  3. Early stopping: VALIDATION")
    print("  4. Ablation Study: VALIDATION")
    print("  5. √âvaluation finale: TEST")
    print("  6. Tests statistiques (McNemar)")
    print("\n" + "="*80 + "\n")

    # Initialisation
    if single_csv is not None:
        pipeline = FairMultimodalAnalysisPipeline(
            single_csv_path=single_csv,
            train_size=train_size,
            val_size=val_size
        )
    else:
        pipeline = FairMultimodalAnalysisPipeline(
            train_csv_path=train_csv,
            val_csv_path=val_csv,
            test_csv_path=test_csv
        )

    # Analyse compl√®te
    pipeline.run_fair_analysis()

    # Ablation study
    pipeline.ablation_study_analysis()

    # √âvaluation finale avec tests statistiques
    pipeline.final_test_evaluation()

    # Sauvegarde
    pipeline.save_results()

    print("\nüéâ ANALYSE COMPL√àTE TERMIN√âE !")
    print("üìÅ Fichiers g√©n√©r√©s:")
    print("   - devign_analysis_results.csv")
    print("   - best_dual_branch_distilled_*.pth")

    return pipeline